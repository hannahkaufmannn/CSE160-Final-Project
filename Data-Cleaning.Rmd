Data-Cleaning.Rmd
CSE-160 Final-Project
Paolo Bartolucci
5/5/2024

Used to load core df used for train/test

[Dataset2](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/about_data)

This is "DataSet2" From the original Report. Dataset 1 contained lat and long data that I was unable to convert to zip code.

This is crash data in NYC. The original dataset is around ~2 mil rows, I used the online filter engine to only include crashes from 2024. Converted the dataset to around 250k rows. There is enough data within the rows to make a difference between fatal vs nonfatal crashes, but that can be changed in the future. For now I will aggregate just a sum of total crashes.

The Result of this is a vector with accident frequencies by zipcode


```{r}


crash_data <- read.csv("data/Motor_Vehicle_collisions.csv")

crash_data<- na.omit(crash_data)

crash_data <- crash_data$ZIP.CODE

crash_data <- as.data.frame(table(crash_data))

names(crash_data) <- c("zipcode","freq")


```

[Dataset3](https://datacommons.techsoup.org/tools/download#pt=CensusZipCodeTabulationArea&place=geoId%2F3651000&sv=Median_Income_Household&dtType=ALL&facets=%7B%7D)

This dataset is the aggregation of existing Cesus data that groups data directly into NYC zip Codes. This contains the median incomes of families by zipcode from 2014-2020.

The income_data df contains the median incomes of NYC zipcodes for 2020.


```{r}
income_data <- read.csv("data/NYC_Census_Income.csv")

income_data <- income_data[,c(2,3,4)]
income_data <- income_data[income_data$Date.Median_Income_Household == 2020,]
income_data <- income_data[,c(1,3)]
names(income_data) <- c("zipcode","median_income")

data <- merge(crash_data, income_data, by="zipcode")
```



[Dataset4](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/data)
This dataset is from the Renthop and Two Sigma rental classication competition. Since we compute our classification via zipcode, we needed to convert latititude and longitude to a zipcode. Any package that does this automatically refers to the Google Maps API, which limits the amount of requests per lattitude and longitude. (Currently in May of 2024 its around 2500 per day). This necessitated a script that manually found the zipcode of a latitude and longitude. 

The way this script works is by using existing GIS (Geographic Information System ) logic and principles. A Shapefile is a glorified ploygon list that uses latitiude and longitude as the coordinate system to plot its shape. The sf library allows for the creation of a multipolygon object, which contains a function to detect if a point exists within a polygon. We create a point with the latitude and longitude as the x and y. Then we parse through the polygon list to detect which polygons contain the point. 

Once the pseudo code for this was written, specific research revealed this is the preferred method of zipcode processing in the GIS community. 

Result notes:
This base dataset was used in a similar but more complex project that added much more data processed preformed its own zipcode processing. I assumed since this project used the Google Maps API (in Python) this was the point of truth with minimal errors. Upon comparison of results, I found that the algorithm we wrote detected existing errors in the heavily processed dataset, and is overall better.


```{r}
library(sf)
library(ggplot2)
crs <- st_crs(4326)
renthop_data <- read.csv("data/renthopNYC.csv")
renthop_data <- renthop_data[,-c(3,4,5,9)]
zip_locations <- read.csv("data/NYC_zipcode_zones.csv")
zip_locations <- zip_locations[,c("MODZCTA","the_geom")]

sf_multipolygons <- st_as_sf(zip_locations, wkt = "the_geom",crs=crs)

incorrect<-0
correct<-0

for(i in 1:nrow(renthop_data))
{
  
  point <- st_point(c(renthop_data$longitude[i],renthop_data$latitude[i]))
  point_with_crs <- st_sfc(point, crs = crs)
  is_within <- st_within(point, sf_multipolygons)
  is_within_vector <- unlist(is_within)
  renthop_data$zipcode[i] <- sf_multipolygons[is_within_vector[1],]$MODZCTA
}


```


Now we merge this data with the other datasets on the zipcode col
```{r}
data<- merge(data, renthop_data, by="zipcode")
```



This is the precleaned data from the github plagarized but can be used to start creating model. NOW DEPRECATED DO NOT RUN
```{r}
rental_data <- read.csv("data/Aggregated_Rental.csv")

rental_target <- rental_data[,c('price','bedrooms','zipcode')]


data<- merge(data, rental_target, by="zipcode")
```


